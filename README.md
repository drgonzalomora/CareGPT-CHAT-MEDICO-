<p align="left">
    &nbspä¸­æ–‡&nbsp ï½œ <a href="README_en.md">English</a>
</p>
<br>

> **warning**
> 
> **CareGPT(åŸåCareLlama) ä¸ºMPUçš„åŒ»ç–—å¤§è¯­è¨€æ¨¡å‹IvyGPTçš„åˆ†æ”¯ï¼Œå…¶å­˜åœ¨æ„ä¹‰æ˜¯æ¢ç´¢åŒ»ç–—æ•°æ®ã€åŒ»ç–—LLMè®­ç»ƒä¸éƒ¨ç½²ç›¸å…³çš„å·¥ä½œç ”ç©¶ã€‚**

<div align="center">
  <a href="https://github.com/WangRongsheng/CareGPT">
    <img src="./assets/images/caregpt.jpg" alt="Logo" height="280">
  </a>

  <p align="center">
    <h3> CareGPT (å…³æ€€GPT)ï¼šåŒ»ç–—LLMï¼Œå¼€æºé©±åŠ¨ï¼Œå…±åˆ›å¥åº·æœªæ¥ </h3>
    <p align="center">
      <em>èµ„æºæ•´åˆ / å¼€æºæ¨¡å‹ / ä¸°å¯Œæ•°æ® / é«˜æ•ˆéƒ¨ç½² </em>
    </p>
    <p align="center">
      <a href='https://github.com/WangRongsheng/CareLlama'>
            <img src='https://img.shields.io/badge/Project-Page-Green'>
      </a>
      <a href='https://github.com/WangRongsheng/CareLlama'>
            <img src='https://img.shields.io/badge/Paper-Arxiv-red'>
      </a>
      <a href="#">
        <img alt="GitHub Contributors" src="https://colab.research.google.com/assets/colab-badge.svg" />
      </a>
      <a href='https://huggingface.co/wangrongsheng'>
        <img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue'>
      </a>
      </br>
      <a href="https://github.com/WangRongsheng/CareLlama/graphs/contributors">
        <img alt="GitHub Contributors" src="https://img.shields.io/github/contributors/WangRongsheng/CareLlama" />
      </a>
      <a href="https://github.com/WangRongsheng/CareLlama/issues">
        <img alt="Issues" src="https://img.shields.io/github/issues/WangRongsheng/CareLlama?color=0088ff" />
      </a>
      <a href="https://github.com/WangRongsheng/CareLlama/pulls">
        <img alt="GitHub pull requests" src="https://img.shields.io/github/issues-pr/WangRongsheng/CareLlama?color=0088ff" />
      </a>
      <a href=href="https://github.com/WangRongsheng/CareLlama/stargazers">
        <img src="https://img.shields.io/github/stars/WangRongsheng/CareLlama?color=ccf">
      </a>
      <a href=href="https://github.com/WangRongsheng/CareLlama">
        <img src="https://img.shields.io/github/repo-size/WangRongsheng/CareLlama.svg?style=flat-square">
      </a>
      </br>
      <a href=href="https://github.com/WangRongsheng/CareLlama">
        <img src="https://visitor-badge.laobi.icu/badge?page_id=https://github.com/WangRongsheng/CareLlama">
      </a>
      <a href=href="https://github.com/WangRongsheng/CareLlama">
        <img src="https://img.shields.io/github/last-commit/WangRongsheng/CareLlama">
      </a>
      <a href="https://github.com/WangRongsheng/CareLlama/blob/main/LICENSE">
        <img alt="GitHub Contributors" src="https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey.svg" />
      </a>
  </p>
</div>

<!--center><kbd><img src="./docs/images/usage.png" height="550px"/></kbd></center-->

<p align="center">
      <a href="#"><strong>è§†é¢‘æ•™ç¨‹</strong></a>
      <a href="https://github.com/WangRongsheng/CareLlama/tree/main#5gradio%E9%83%A8%E7%BD%B2"><strong>å®‰è£…éƒ¨ç½²</strong></a>
      <a href="https://huggingface.co/spaces/wangrongsheng/CareLlama"><strong>åœ¨çº¿ä½“éªŒ</strong></a>
</p>

![](./assets/images/hx.png)

âš¡CaracterÃ­sticas:

Se agregÃ³ la implementaciÃ³n de ajuste de ChatGPT y recomendÃ³ amigos con crÃ©ditos para realizar experimentos de ajuste en ChatGPT;
Admite el modelo de ajuste de implementaciÃ³n ChatGPT-Next-Web ;
Admite modelos de ajuste fino de implementaciÃ³n de Gradio ;
Admite entrenamiento de modelos de serie completa LLaMA y LLaMA-2;
Admite LoRA y QLoRA, incluida la posterior capacitaciÃ³n de aprendizaje reforzado de PPO y DPO;
Admite preguntas y respuestas combinadas con modelos y base de conocimientos;
InformaciÃ³n de material de orientaciÃ³n mÃ©dica de cÃ³digo abierto para mÃ¡s de 60 departamentos hospitalarios ;
Se desarrollÃ³ una herramienta para respaldar la destilaciÃ³n de datos mÃ©dicos del modelo GPT-4/ChatGPT , que puede generar por lotes diversos datos para crear una base de conocimientos y realizar ajustes;
Agrega una gran cantidad de LLM mÃ©dicos de cÃ³digo abierto, datos mÃ©dicos para capacitaciÃ³n de LLM, datos de implementaciÃ³n de LLM, evaluaciÃ³n de LLM y compilaciÃ³n de recursos de LLM relacionados;
Participamos en la evaluaciÃ³n de la lista CMB de LLM mÃ©dicos - IvyGPT . En la prueba, estÃ¡bamos por delante de ChatGPT y varios LLM mÃ©dicos de cÃ³digo abierto;
Contamos con mÃºltiples LLM mÃ©dicos de cÃ³digo abierto capacitados en diferentes LLM bÃ¡sicos basados â€‹â€‹en nuestros propios conjuntos de datos. Puede descargarlos directamente para experimentar;
ğŸConjunto de datos 
Datos previos al entrenamiento 
LLM-Pretrain-FineTune/data_pretrain
MÃ©dicoGPT/preentrenamiento
zyj
TCM-Ancient-Books (casi 700 textos antiguos de medicina china)
Datos de entrenamiento supervisados 
icliniq-10k(es)
HealthCareMagic-100k(es)
ShenNong_TCM_Dataset
âœ… ChatMed_Consult_Dataset
Datos-de-diÃ¡logo-mÃ©dico-chino
cMedQA2
âœ… Huatuo-26M
cMedQA2
webMedQA
PubMedQA
CMCQA
âœ… QiZhenGPT
âœ… LLM-Pretrain-FineTune/data_sft
Sistema de diÃ¡logo mÃ©dico
IMCS-V2
CHIP-MDCFNPC
MedDG
âœ… HuatuoGPT-sft-datos-v1
MÃ©dicoGPT/ajuste fino
âœ… shibing624/mÃ©dico
medAlpaca/datos
âœ…Zhongjing /sft
diÃ¡logo_medico
huatuo_enciclopedia_qa
Med-ChatGLM/datos
CMB
GenMedGPT-5k(es)
Alpaca-CoT(general)
âœ… DISCO-Med-SFT
Datos de entrenamiento de recompensa 
GPT mÃ©dico/recompensa
Zhongjing/rw
comparaciÃ³n_gpt4_data
HH-RLHF
Ultrafeedback
ğŸ—œï¸CapacitaciÃ³n de proceso completo 
1. Instalar dependencias 
conda create -n llm python=3.11
conda activate llm
python -m pip install -r requirements.txt
Descarga del modelo LLaMA: https://blog.csdn.net/u014297502/article/details/129829677
# è½¬ä¸ºHFæ ¼å¼
python -m transformers.models.llama.convert_llama_weights_to_hf \
    --input_dir path_to_llama_weights --model_size 7B --output_dir path_to_llama_model
Descarga del modelo LLaMA-2: https://huggingface.co/meta-llama
2.ConfiguraciÃ³n de datos 
ConfiguraciÃ³n del conjunto de datos, formato de datos PT, SFT, RW
3. ConfiguraciÃ³n de entrenamiento 
ParÃ¡metros e instrucciones de entrenamiento.
4. ConfiguraciÃ³n de inferencia 
ParÃ¡metros e instrucciones de inferencia.
5.ImplementaciÃ³n de Gradio 
Instrucciones de implementaciÃ³n de Gradio
6. ImplementaciÃ³n de ChatGPT-Next-Web
Siguientes instrucciones de implementaciÃ³n
ğŸ’«Experiencia prÃ¡ctica 
En CareGPT, la segmentaciÃ³n de palabras chinas no se agrega ni se reentrena al modelo de segmentaciÃ³n de palabras, pero el efecto sigue siendo prometedor;
Todo el proceso de formaciÃ³n de LLM incluye: formaciÃ³n previa, ajuste fino supervisado, modelo de recompensa y aprendizaje reforzado. En la mayorÃ­a de los casos, el ajuste fino supervisado puede satisfacer sus propias necesidades ;
Cuando la potencia informÃ¡tica es suficiente, se recomienda utilizar datos mÃ©dicos y datos de corpus generales para la capacitaciÃ³n , de modo que el modelo no solo pueda tener capacitaciÃ³n y aprendizaje mÃ©dicos, sino que tambiÃ©n mantenga capacidades generales (como seguir instrucciones);
No espere que un LLM mÃ©dico pueda satisfacer todas las necesidades. Un enfoque razonable puede ser una base de conocimientos actualizada en tiempo real + un LLM mÃ©dico ajustado (como ChatLaw );
La serie de modelos BLOOMZ se entrenÃ³ utilizando el corpus PILE, que contiene varios textos mÃ©dicos, incluidos PubMed Central, PubMed Abstractsetc. Estos valiosos textos han enriquecido enormemente el sistema de conocimiento mÃ©dico del modelo BLOOMZ, por lo que muchos proyectos de cÃ³digo abierto darÃ¡n prioridad a BLOOMZ como modelo base para el ajuste mÃ©dico;
(2023.08.26) ChatGPT estÃ¡ entrenado en base a Code GPT. Â¿Usaremos CodeLLaMA para ajustar las tareas posteriores para lograr mejores resultados que el ajuste fino en LLaMA-1/2?
La combinaciÃ³n de nuestro trabajo reciente con muchos trabajos publicados recientemente demuestra: en la era LLM, la è´¨é‡ > æ•°é‡verdad de los datos, como: Â¡ Menos es mÃ¡s! Entregado a Qingyuan && Caspian | Uso de 200 datos para ajustar el modelo, superando MiniGPT-4 ! , los datos SFT a gran escala debilitarÃ¡n el LLM de tareas posteriores o perderÃ¡n ICL, CoT y otras capacidades;
Para los modelos verticales, tal vez deberÃ­amos prestar mÃ¡s atenciÃ³n al proceso PT en lugar de recopilar millones de datos SFT para capacitaciÃ³n. Nuestra sugerencia es å¤§è§„æ¨¡é¢„è®­ç»ƒ+å°è§„æ¨¡ç›‘ç£å¾®è°ƒ=è¶…å¼ºçš„LLMæ¨¡å‹;
AÃºn no se ha abierto un buen LLM mÃ©dico previamente capacitado en la comunidad de cÃ³digo abierto, y espero que alguien pueda complementar ese trabajo;
Â¿La capacitaciÃ³n previa puede infundir conocimiento, mientras que el ajuste fino supervisado solo activa las capacidades del dominio (no puede centrarse en el conocimiento)? Â¿El conocimiento previo a la capacitaciÃ³n deberÃ­a reflejar el conocimiento de ajuste supervisado? Â¿Las decenas de GB de conocimiento del corpus pre-entrenado se verÃ¡n abrumadas por el conocimiento del modelo original pre-entrenado de billones de tokens?
El entrenamiento previo secundario de una gran cantidad de datos requiere comparar varios tipos de otros datos: (1) Una vez completado el entrenamiento del modelo de lenguaje, se han determinado las partes responsables de cada Ã¡rea de los parÃ¡metros. que no estÃ¡ disponible durante el entrenamiento previo, los parÃ¡metros aumentarÃ¡n. Los cambios de amplitud causan la pÃ©rdida de toda la capacidad del modelo de lenguaje; (2) Para el entrenamiento previo secundario de datos a gran escala, de 5 a 10 veces los datos en el original la capacitaciÃ³n previa debe agregarse, mezclarse y capacitarse en conjunto;
La fase de ajuste de la instrucciÃ³n no puede llevar a cabo demasiadas rondas de capacitaciÃ³n: (1) Entrenar mÃºltiples EPOCH en una pequeÃ±a cantidad de datos puede causar cambios en Ã¡reas clave del lenguaje, lo que lleva a la falla de todo el modelo; (2) Multa de instrucciÃ³n -ajuste para mejoras de tareas especÃ­ficas. Para garantizar que las Ã¡reas clave de las capacidades del lenguaje del modelo no se ajusten significativamente, es necesario agregar datos de ajuste fino de instrucciÃ³n general o datos de preentrenamiento;
Los datos de entrenamiento deben controlar estrictamente el ruido: (1) Si aparece una pequeÃ±a cantidad de datos de ruido continuo en los datos previos al entrenamiento, como repeticiÃ³n continua de palabras, secuencias sin palabras, etc., puede provocar ajustes en dimensiones especÃ­ficas, lo que provocarÃ¡ el PPL general del modelo fluctÃºa significativamente; (2) Si hay una gran cantidad de fragmentos de instrucciÃ³n en las instrucciones de ajuste fino supervisadas que no coinciden con el modelo de lenguaje grande original, tambiÃ©n puede hacer que el modelo ajuste dimensiones especÃ­ficas, reduciendo asÃ­ significativamente el rendimiento general del modelo;
Al ajustar un modelo grande con datos mixtos de mÃºltiples capacidades, aparecerÃ¡: alto conflicto de recursos y baja ganancia de recursos, por lo que mezclar diferentes datos para el ajuste fino requiere ciertas habilidades de ingenierÃ­a;
En tÃ©rminos generales, existe una diferencia de rendimiento no despreciable entre lora y el ajuste completo (por ejemplo, LoRA da como resultado un rendimiento entre un 4% y un 6% menor en comparaciÃ³n con el ajuste fino completo );
Importante

Â¡ Todos son bienvenidos a agregar nuevas experiencias a ISSUE !

11, 12, 13 La metodologÃ­a proviene del modelo de 13 mil millones de lenguajes grandes. Â¡Cambiar solo un peso perderÃ¡ por completo la capacidad del lenguaje! Las Ãºltimas investigaciones del Laboratorio de Procesamiento del Lenguaje Natural de la Universidad de Fudan .

14 MetodologÃ­a de cÃ³mo las habilidades en modelos de lenguaje grandes se ven afectadas por la composiciÃ³n de datos de ajuste supervisado

ğŸ§°Modelo de cÃ³digo abierto 
escenario	IntroducciÃ³n a las pesas	enlace de descarga	CaracterÃ­sticas	modelo base	mÃ©todo de ajuste fino	conjunto de datos
ğŸŒŸSupervisiÃ³n y puesta a punto	Los datos de diÃ¡logo de varios turnos se entrenan en base a LLaMA2-7b-Chat	âš™ï¸CareLlama2-7b-chat-sft-multi , ğŸ§°CareLlama2-7b-multi	Excelentes habilidades de conversaciÃ³n en varios turnos.	LLaMA2-7b-Chat	QLoRA	milÃ­metros
Supervisar el ajuste	Se entrenan datos ricos y eficientes del diÃ¡logo mÃ©dico-paciente en base a LLaMA2-7b-Chat	âš™ï¸CareLlama2-7b-chat-sft-med	Excelentes capacidades de diagnÃ³stico de enfermedades del paciente.	LLaMA2-7b-Chat	QLoRA	mmm
Supervisar el ajuste	Los datos mixtos se entrenan en base a LLaMA-7b.	âš™ï¸CareLlama1-7b-merge	Mejores habilidades de conversaciÃ³n mÃ©dica.	Llama-7b	lora	mmmmm
Supervisar el ajuste	Los datos mixtos se entrenan en base a LLaMA2-7b-Chat.	âš™ï¸CareLlama2-7b-merge , ğŸ§°CareLlama2-7b-merge-mix	Mejores habilidades de conversaciÃ³n mÃ©dica.	LLaMA2-7b-Chat	QLoRA	mmmmm
DPO		âš™ï¸CareLlama2-7b-merge-dpo				rlhh
Supervisar el ajuste	Se entrenan mÃ¡s datos mixtos basados â€‹â€‹en LLaMA2-7b-Chat	âš™ï¸CareLlama2-7b-super , ğŸ§°CareLlama2-7b-super-mix	Mejores habilidades de conversaciÃ³n mÃ©dica.	LLaMA2-7b-Chat	QLoRA	mm,ls,ks,mc,sra,qz,hm
Supervisar el ajuste	Los datos de diÃ¡logo de varios turnos se entrenan en base a Baichuan-13B-Chat	âš™ï¸Baichuan-13B-Chat-sft-multi	Excelentes habilidades de conversaciÃ³n en varios turnos.	Baichuan-13B-Chat	QLoRA	milÃ­metros
Supervisar el ajuste	Los datos de la conversaciÃ³n mixta se entrenan en base a Baichuan-13B-Chat	âš™ï¸Baichuan-13B-Chat-sft-merge	Mejores habilidades de diÃ¡logo mÃ©dico-paciente	Baichuan-13B-Chat	QLoRA	mmmmm
Supervisar el ajuste	Los datos de la conversaciÃ³n mixta se entrenan en base a Baichuan-13B-Chat	âš™ï¸Baichuan-13B-Chat-sft-super , ğŸ§°Baichuan-13B-Chat-sft-super-mix	Mejores habilidades de diÃ¡logo mÃ©dico-paciente	Baichuan-13B-Chat	QLoRA	mm,ls,ks,mc,sra,qz,hm
ğŸŒŸSupervisiÃ³n y puesta a punto	Los datos de diÃ¡logo de mÃºltiples turnos se entrenan en base a QWen-7B	ğŸ§°carellm	Excelentes habilidades de conversaciÃ³n en varios turnos.	QWen-7B	QLoRA	milÃ­metros
Supervisar el ajuste	Los datos de conversaciones de varios turnos se entrenan en funciÃ³n de QWen-14B-Chat	âš™ï¸careqwen-14B-Chat-sft-multi	Excelentes habilidades de conversaciÃ³n en varios turnos.	QWen-14B-Chat	QLoRA	milÃ­metros
Supervisar el ajuste	Los datos de diÃ¡logo de varios turnos se entrenan en base a InternLM-20B-Chat	âš™ï¸careinternlm-20B-Chat-sft-multi , ğŸ§°careinternlm-20B-Chat-sft-multi-mix	Excelentes habilidades de conversaciÃ³n en varios turnos.	PasanteLM-20B-Chat	QLoRA	milÃ­metros
ğŸŒŸSupervisiÃ³n y puesta a punto	Los datos del diÃ¡logo de rondas mÃºltiples se entrenan en base a Baichuan2-13B-Chat	âš™ï¸Baichuan2-13B-Chat-sft-multi , ğŸ§°Baichuan2-13B-Chat-sft-multi-mix	Excelentes habilidades de conversaciÃ³n en varios turnos.	Baichuan2-13B-Chat	QLoRA	milÃ­metros
CÃ³mo utilizar :

Descargue el modelo base correspondiente;
Si es LLaMA se convertirÃ¡ a formato HF , si es LLaMA-2 y la descarga es en formato HF no se requiere conversiÃ³n;
Descargue los pesos que desea cargar arriba;
Comience a usar nuestro modelo basado en la configuraciÃ³n de inferencia ;
ğŸ’¯EvaluaciÃ³n del modelo 
Modelo	InstituciÃ³n	Puntaje
ShuKunGPT	TecnologÃ­a Shukun	64,44
GPT-4	AbiertoAI	58,37
Baichuan2-53B	Inteligencia de Baichuan	45,69
ChatGLM2-6B	Espectro de sabidurÃ­a IA	44,91
Baichuan-13B-chat	Inteligencia de Baichuan	41,63
IvyGPT (Baichuan2-13B+10W)	Universidad PolitÃ©cnica de Macao	38,54
ChatGPT	AbiertoAI	38.09
IvyGPT (Baichuan-13B+10W)	Universidad PolitÃ©cnica de Macao	34,60
ChatGLM3-6B	Espectro de sabidurÃ­a IA	33,76
HuatuoGPT (BLOOMZ)	La Universidad China de Hong Kong (Shenzhen)	31.38
IvyGPT (Qwen-7B+PT-WiNGPT3.2 mil millones+10W)	Universidad PolitÃ©cnica de Macao	28.26
MÃ©dicoGPT	-	26,45
ChatMed-Consulta	Universidad Normal del Este de China	21.71
Bentsao	Instituto de TecnologÃ­a de Harbin	21.25
ChatGLM-Med	Instituto de TecnologÃ­a de Harbin	20.67
IvyGPT (LLaMA-2-7B+220W)	Universidad PolitÃ©cnica de Macao	18.55
DoctorGLM	Universidad TecnolÃ³gica de Shanghai	7.63
BianQue-2	Universidad Normal del Este de China	7.26
Modelo	Tasa de no alucinaciones
ERNIE-Bot	69,33%
Baichuan2-53B	68,22%
ChatGLM-Pro	61,33%
GPT-4-0613	53,11%
QWen-14B-Chat	46,89%
Baichuan2-13B-Chat	42,44%
Baichuan2-7B-Chat	40,67%
GPT3.5-turbo-0613	39,33%
ChatGLM2-6B	34,89%
Base Baichuan2-13B	33,78%
Baichuan-13B-Chat	31,33%
Base Baichuan-13B	25,33%
Base Baichuan2-7B	25,33%
Base Baichuan-7B	22,22%
Referenciado desde: 2310.03368.pdf

ğŸ“³PresentaciÃ³n de resultados 


Ver mÃ¡s demostraciones
ğŸ°Descargo de responsabilidad 
Los recursos relacionados con este proyecto son Ãºnicamente para investigaciÃ³n acadÃ©mica y estÃ¡n estrictamente prohibidos para uso comercial. Cuando utilice piezas que incluyan cÃ³digo de terceros, siga estrictamente el acuerdo de cÃ³digo abierto correspondiente. El contenido generado por el modelo se ve afectado por factores como el cÃ¡lculo del modelo, la aleatoriedad y la pÃ©rdida de precisiÃ³n de la cuantificaciÃ³n, y este proyecto no puede garantizar su precisiÃ³n. Incluso si el resultado del modelo de este proyecto se ajusta a hechos mÃ©dicos, no puede utilizarse como base para un diagnÃ³stico mÃ©dico real. Este proyecto no asume ninguna responsabilidad legal por el contenido generado por el modelo, ni es responsable de las pÃ©rdidas que puedan surgir del uso de recursos relacionados y resultados de producciÃ³n.

ğŸ¥‚Referencia del proyecto 
Este trabajo fue completado por Wang Rongsheng, Zhou Ruizhe y Chen Haoming, estudiantes de maestrÃ­a de la Facultad de Ciencias Aplicadas de la Universidad PolitÃ©cnica de Macao, y los supervisores fueron el profesor asociado Tan Tao y el profesor asociado Wang Yapeng.

Si utiliza el modelo, datos o cÃ³digo de este proyecto, por favor declare la siguiente referencia:

@misc{wang2023caregpt,
      title={CareGPT: Medical LLM, Open Source Driven for a Healthy Future}, 
      author={Rongsheng Wang, Ruizhe Zhou, Haoming Chen, Yapeng Wang, Tao Tan},
      year={2023},
      publisher = {GitHub},
      journal = {GitHub repository},
      howpublished = {\url{https://github.com/WangRongsheng/CareGPT}},
}
@article{wang2023ivygpt,
  title={IvyGPT: InteractiVe Chinese pathwaY language model in medical domain},
  author={Wang, Rongsheng and Duan, Yaofei and Lam, ChanTong and Chen, Jiexi and Xu, Jiangsheng and Chen, Haoming and Liu, Xiaohong and Pang, Patrick Cheong-Iao and Tan, Tao},
  journal={arXiv preprint arXiv:2307.10512},
  year={2023}
}
@Misc{llama-factory,
  title = {LLaMA Factory},
  author = {hiyouga},
  howpublished = {\url{https://github.com/hiyouga/LLaMA-Factory}},
  year = {2023}
}
Este proyecto ha sido incluido en Heart of the Machine SOTA-CareGPT .



ğŸ””Licencia 
Este repositorio tiene la licencia MIT , consulte los tÃ©rminos de la licencia.

ğŸ—ï¸Apoyo de patrocinio 
Si cree que este proyecto es Ãºtil para usted y estÃ¡ dispuesto a apoyar nuestro trabajo, puede hacerlo de las siguientes maneras ( dÃ©jenos su ID de WeChat ):

	
Su apoyo serÃ¡ nuestra motivaciÃ³n para continuar explorando LLM, y todo el apoyo se utilizarÃ¡ para las tareas de implementaciÃ³n de inferencia de modelos .

ğŸ“šReferencia del proyecto 
MaestrÃ­a en Medicina 
https://github.com/llSourcell/DoctorGPT
https://github.com/facebookresearch/llama-recipes
https://github.com/Kent0n-Li/ChatDoctor
https://github.com/michael-wzhu/ShenNong-TCM-LLM
https://github.com/michael-wzhu/ChatMed
https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese
https://github.com/SCIR-HI/Med-ChatGLM
https://github.com/xionghonglin/DoctorGLM
https://github.com/MediaBrain-SJTU/MING
https://github.com/CMKRG/QiZhenGPT
https://github.com/NLPxiaoxu/LLM-Pretrain-FineTune
https://github.com/scutcyr/BianQue
https://github.com/thomas-yanxin/Sunsimiao
https://github.com/kbressem/medAlpaca
https://github.com/FreedomIntelligence/HuatuoGPT
https://github.com/shibing624/MedicalGPT
https://github.com/chaoyi-wu/PMC-LLaMA
https://github.com/parischang/CMLM-ZhongJing
https://github.com/SupritYoung/Zhongjing
https://github.com/openmedlab/PULSE
https://github.com/FudanDISC/DISC-MedLLM
https://github.com/Zlasejd/HuangDI
https://github.com/2020MEAI/TCMLLM
https://github.com/PharMolix/OpenBioMed
https://huggingface.co/Writer/palmyra-med-20b
https://github.com/taininghealth/WiNGPT2
https://github.com/DUTIR-BioNLP/Taiyi-LLM
https://github.com/TONYCHANBB/HealGPT
https://github.com/som-shahlab/Clinfo.AI
https://github.com/DUTIR-BioNLP/Taiyi-LLM
Revisar LLM 
https://github.com/FreedomIntelligence/CMB
Experiencia LLM 
https://medical.chat-data.com/
http://med.fudan-disc.com/
https://www.huatuogpt.cn/
https://huggingface.co/spaces/wangrongsheng/CareLlama
( contraseÃ±a ) https://huggingface.co/spaces/fb700/chatglm-fitness-RLHF
http://heal-gpt.cn/
TecnologÃ­a Sensetime-Dayi: https://chat.sensetime.com/
iFlytek-iFlytek Xiaoyi: bÃºsqueda y uso de pequeÃ±os programas
https://www.cinfo.ai/
Ali Tongyi Renxin: https://tongyi.aliyun.com/renxin
Implementar LLM 
https://github.com/a16z-infra/llama2-chatbot
https://github.com/liltom-eth/llama2-webui
https://github.com/soulteary/docker-llama2-chat
https://huggingface.co/spaces/LinkSoul/Chinese-Llama-2-7b
https://github.com/mushan0x0/AI0x0.com
https://github.com/Yidadaa/ChatGPT-Next-Web
https://github.com/sunner/ChatALL
https://github.com/chatchat-space/Langchain-Chatchat
https://github.com/wenda-LLM/wenda
https://github.com/xusenlinzy/api-for-open-llm
https://github.com/yuanjie-ai/ChatLLM
https://github.com/labring/FastGPT
https://github.com/vllm-project/vllm
https://github.com/dataelement/bisheng
https://github.com/lobehub/lobe-chat
https://github.com/purton-tech/bionicgpt
https://github.com/Chainlit/chainlit
https://github.com/arc53/DocsGPT
https://vercel.com/templates/ai
https://github.com/ollama-webui/ollama-webui
https://github.com/huggingface/chat-ui
https://github.com/xusenlinzy/api-for-open-llm
ProducciÃ³n de datos LLM 
https://github.com/yanqiangmiffy/GoGPT-Instruction
https://github.com/wpydcr/LLM-Kit
https://github.com/huang1332/finetune_dataset_maker
https://github.com/tresColorFr/LLMforDialogDataGenerate
https://github.com/alibaba/data-juicer
https://github.com/duanyu/LabelFast
Recursos de maestrÃ­a en Derecho 
https://github.com/onejune2018/Awesome-Medical-Healthcare-Dataset-For-LLM
https://github.com/WangRongsheng/MedQA-ChatGLM
https://github.com/hiyouga/LLaMA-Efficient-Tuning
https://github.com/WangRongsheng/Use-LLMs-in-Colab
https://github.com/HqWu-HITCS/Awesome-Chinese-LLM
https://github.com/LearnPrompt/LLMs-cookbook
https://github.com/liucongg/ChatGPTBook
https://github.com/EvilPsyCHo/train_custom_LLM
LLM multimodal 
https://github.com/QwenLM/Qwen-VL
https://github.com/haotian-liu/LLaVA
https://github.com/kyegomez/Med-PaLM
https://github.com/williamliujl/Qilin-Med-VL
Descargar LLM 
https://huggingface.co/
https://modelscope.cn/
https://aliendao.cn/
https://hf-mirror.com/


Acerca de
ğŸŒ CareGPT (CareGPT) es un modelo de lenguaje mÃ©dico a gran escala que, al mismo tiempo, integra docenas de conjuntos de datos de ajuste mÃ©dico disponibles pÃºblicamente y modelos de lenguaje mÃ©dico a gran escala abiertos disponibles, incluida la capacitaciÃ³n, evaluaciÃ³n e implementaciÃ³n de LLM para promover la Se desarrolla un rÃ¡pido desarrollo de la LLM mÃ©dica. LLM mÃ©dico, cÃ³digo abierto impulsado por un futuro saludable.

Temas
llama gpto baichuan modelos de lenguaje grande llama2 llm medico
Recursos
LÃ©ame
Licencia
licencia MIT
 Actividad
Estrellas
 87 estrellas
Vigilantes
 4 mirando
tenedores
 14 tenedores
Repositorio de informes
Idiomas
PitÃ³n
99,9%
 
CSS
0,1%
Pie de pÃ¡gina
Â© 2023 GitHub, Inc.
NavegaciÃ³n de pie de pÃ¡gina
TÃ©rminos
Privacidad
Seguridad
Estado
Documentos
Contacto GitHub
Precios
API
CapacitaciÃ³n
Blog
Acerca de
